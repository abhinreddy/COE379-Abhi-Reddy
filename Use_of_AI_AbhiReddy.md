# Use of AI

I created this document to list each place I used AI to help me in my **Project 1** notebook.  
In the notebook, I referenced these with comments like: `# The code below was helped by AI-[n]`.

---

[1]. **Tool:** ChatGPT  
**Prompt:** “Set up the Python environment for pandas/numpy/matplotlib and scikit-learn (split, pipeline, KNN, LogisticRegression, metrics), include `display`, and fix a reproducible random seed.”  
**Output:**
```python
# The code below was generated by AI; see [1].
import os, re, math, numpy as np, pandas as pd, matplotlib.pyplot as plt
from IPython.display import display

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

pd.set_option("display.max_columns", 120)
pd.set_option("display.width", 120)

RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("Setup complete.")
```

---

[2]. **Tool:** ChatGPT  
**Prompt:** “Write a robust loader that accepts either a web URL (raw GitHub) or a local file path; print the loaded shape and show a small head.”  
**Output:**
```python
# The code below was generated by AI; see [2].
def _is_url(p): return str(p).lower().startswith(("http://","https://","ftp://"))

DATA_PATH = "https://raw.githubusercontent.com/joestubbs/coe379L-fa25/main/datasets/unit01/project1.csv"  # or local "project1.csv"

try:
    if _is_url(DATA_PATH):
        df_raw = pd.read_csv(DATA_PATH, encoding="utf-8")
        print(f"Loaded from URL: {DATA_PATH}")
    else:
        if not os.path.exists(DATA_PATH):
            raise FileNotFoundError(f"Local file not found: {DATA_PATH}")
        df_raw = pd.read_csv(DATA_PATH, encoding="utf-8")
        print(f"Loaded local file: {DATA_PATH}")
except Exception as e:
    raise RuntimeError(f"Failed to load dataset from {DATA_PATH}: {e}")

print("Loaded shape:", df_raw.shape)
display(df_raw.head(3))
```

---

[3]. **Tool:** ChatGPT  
**Prompt:** “After loading, show shape, dtypes, memory usage, duplicate count; drop duplicates into a working DataFrame `df`.”  
**Output:**
```python
# The code below was generated by AI; see [3].
print("Shape:", df_raw.shape)
print("\nDtypes:"); print(df_raw.dtypes)
print("\nMemory usage (MB):", df_raw.memory_usage(deep=True).sum() / (1024**2))

dup_count = df_raw.duplicated().sum()
print("\nDuplicate rows:", dup_count)

df = df_raw.drop_duplicates().copy()
print("After dropping duplicates:", df.shape)
```

---

[4]. **Tool:** ChatGPT  
**Prompt:** “Helpers to (a) convert ‘Age Upon Outcome’ strings to **days** (float); (b) parse ‘Sex Upon Outcome’ into (sex, intact-status).”  
**Output:**
```python
# The code below was generated by AI; see [4].
def parse_age_to_days(age_str):
    if not isinstance(age_str, str) or not age_str.strip():
        return np.nan
    s = age_str.strip().lower()
    parts = s.split()
    if len(parts) < 2:
        try: return float(s)
        except: return np.nan
    import re
    try:
        value = float(re.sub(r"[^0-9.]", "", parts[0]))
    except:
        return np.nan
    unit = parts[1]
    if unit.startswith("year"):  return value * 365.0
    if unit.startswith("month"): return value * 30.4
    if unit.startswith("week"):  return value * 7.0
    if unit.startswith("day"):   return value
    return np.nan

def parse_sex_upon_outcome(sex_str):
    if not isinstance(sex_str, str) or not sex_str.strip():
        return "unknown", "unknown"
    s = sex_str.strip().lower()
    sex = "male" if "male" in s else ("female" if "female" in s else "unknown")
    intact = "intact" if "intact" in s else ("altered" if ("neutered" in s or "spayed" in s or "altered" in s) else "unknown")
    return sex, intact
```

---

[5]. **Tool:** ChatGPT  
**Prompt:** “A single **clean + feature engineering** cell that: normalizes headers; parses dates; restricts to {Adoption, Transfer}; builds `AgeAtOutcomeDays`, `Sex_simple`, `Intact_status`, `HasName`, time features; imputes numeric/categorical; and is safe to re-run (no chained-assignment warnings).”  
**Output:**
```python
# The code below was generated by AI; see [5].
df_raw.columns = df_raw.columns.str.strip()
df_raw = df_raw.rename(columns={
    "Animal ID": "AnimalID",
    "Outcome Type": "OutcomeType",
    "Outcome Subtype": "Outcome Subtype",
    "Animal Type": "Animal Type",
    "Sex upon Outcome": "Sex Upon Outcome",
    "Age upon Outcome": "Age Upon Outcome",
})

df = df_raw.drop_duplicates().copy()

# Parse dates / derive time parts
if "DateTime" in df.columns:
    dt = pd.to_datetime(df["DateTime"], errors="coerce", utc=True)
    df["OutcomeYear"]  = dt.dt.year
    df["OutcomeMonth"] = dt.dt.month
    df["OutcomeDOW"]   = dt.dt.dayofweek
    df["OutcomeHour"]  = dt.dt.hour

if "Date of Birth" in df.columns:
    df["Date of Birth"] = pd.to_datetime(df["Date of Birth"], errors="coerce")

if "MonthYear" in df.columns:
    try:
        df["MonthYear"] = pd.to_datetime(df["MonthYear"], format="%b-%y", errors="coerce")
    except Exception:
        df["MonthYear"] = pd.to_datetime(df["MonthYear"], errors="coerce")

# Keep only Adoption / Transfer
if "OutcomeType" in df.columns:
    kept_before = len(df)
    df = df[df["OutcomeType"].isin(["Adoption", "Transfer"])].copy()
    print(f"Filtered OutcomeType to Adoption/Transfer: kept {len(df)} of {kept_before} rows.")

# Feature engineering
df["AgeAtOutcomeDays"] = df.get("Age Upon Outcome", pd.Series(index=df.index, dtype=object))                              .apply(parse_age_to_days).astype(float)

sex_series = df.get("Sex Upon Outcome", pd.Series(index=df.index, dtype=object)).astype("string")
sex_intact = sex_series.apply(parse_sex_upon_outcome)
df["Sex_simple"]    = sex_intact.apply(lambda s: s[0])
df["Intact_status"] = sex_intact.apply(lambda s: s[1])

# Missingness / imputations
df["HasName"] = (
    df.get("Name", pd.Series(index=df.index, dtype=object))
      .astype("string").str.strip().replace({"": np.nan}).notna()
)
df["AgeAtOutcomeDays"] = df["AgeAtOutcomeDays"].fillna(df["AgeAtOutcomeDays"].median(skipna=True))

for col in ["Sex_simple", "Intact_status", "Animal Type", "OutcomeType"]:
    if col in df.columns:
        s = df[col].astype("string").str.strip().replace({"": "unknown", "nan": "unknown"}).fillna("unknown")
        df[col] = pd.Categorical(s)

df_eda = df.copy()
print("Columns now:", list(df.columns))
print("Shape:", df.shape)
display(df.head(3))
```

---

[6]. **Tool:** ChatGPT  
**Prompt:** “Univariate plots for OutcomeType, Animal Type, Sex_simple, and a histogram for AgeAtOutcomeDays.”  
**Output:**
```python
# The code below was generated by AI; see [6].
if "OutcomeType" in df_eda.columns:
    ax = df_eda["OutcomeType"].value_counts().sort_values(ascending=False).plot(kind="bar", title="OutcomeType Counts")
    ax.set_xlabel("OutcomeType"); ax.set_ylabel("Count"); plt.show()

if "Animal Type" in df_eda.columns:
    ax = df_eda["Animal Type"].value_counts().sort_values(ascending=False).head(10).plot(kind="bar", title="Top Animal Types")
    ax.set_xlabel("Animal Type"); ax.set_ylabel("Count"); plt.show()

if "Sex_simple" in df_eda.columns:
    ax = df_eda["Sex_simple"].value_counts().sort_values(ascending=False).plot(kind="bar", title="Sex (simplified)")
    ax.set_xlabel("Sex"); ax.set_ylabel("Count"); plt.show()

if "AgeAtOutcomeDays" in df_eda.columns:
    ax = df_eda["AgeAtOutcomeDays"].plot(kind="hist", bins=40, title="AgeAtOutcomeDays (hist)")
    ax.set_xlabel("AgeAtOutcomeDays"); plt.yscale("linear"); plt.show()
```

---

[7]. **Tool:** ChatGPT  
**Prompt:** “Drop irrelevant columns for modeling (IDs, names, raw dates, MonthYear, Outcome Subtype, Color); set key columns to category; show dtypes.”  
**Output:**
```python
# The code below was generated by AI; see [7].
df_model = df.copy().drop_duplicates()

drop_cols = []
for c in ["AnimalID","Name","Outcome Subtype","Date of Birth","DateTime","MonthYear","Color"]:
    if c in df_model.columns:
        drop_cols.append(c)
df_model = df_model.drop(columns=drop_cols, errors="ignore")
print("Dropped columns:", drop_cols)

for c in df_model.columns:
    if c in ["OutcomeType","Animal Type","Sex_simple","Intact_status","Breed"]:
        df_model[c] = df_model[c].astype("category")

try: display(df_model.head(3))
except: print(df_model.head(3))
print("Current dtypes:\n", df_model.dtypes)
```

---

[8]. **Tool:** ChatGPT  
**Prompt:** “**Drop Breed before encoding**; one-hot encode the remaining categoricals (leave OutcomeType out), build X and y, and do a stratified 80/20 split.”  
**Output:**
```python
# The code below was generated by AI; see [8].
import numpy as np

df_enc = df_model.copy()
if "Breed" in df_enc.columns:
    df_enc = df_enc.drop(columns=["Breed"])  # per assignment, drop before encoding

categorical_cols = [c for c in df_enc.select_dtypes(["category","object"]).columns if c != "OutcomeType"]
df_enc = pd.get_dummies(df_enc, columns=categorical_cols, drop_first=False)

print("Encoded shape:", df_enc.shape)
print("Any breed dummies left?", any(col.lower().startswith("breed_") for col in df_enc.columns))

X = df_enc.drop(columns=["OutcomeType"]).astype(np.float32, copy=False)
y = df["OutcomeType"].astype(str).values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y
)
print("Train:", X_train.shape, "Test:", X_test.shape)
```

---

[9]. **Tool:** ChatGPT  
**Prompt:** “Models cell that: imputes NaNs; scales; trains KNN baseline; runs a **small, memory-safe** GridSearchCV (3-fold × 4 candidates); and trains Logistic Regression with higher `max_iter` to avoid warnings; print accuracy/precision/recall/F1 + confusion matrices.”  
**Output:**
```python
# The code below was generated by AI; see [9].
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

def evaluate_and_print(name, y_true, y_pred):
    print(f"\n=== {name} ===")
    print("Accuracy:", f"{accuracy_score(y_true, y_pred):.4f}")
    print("\nClassification Report:\n", classification_report(y_true, y_pred, digits=4))
    import numpy as np
    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
    print("Confusion Matrix (labels order:", list(np.unique(y_true)), "):\n", cm)

# KNN baseline
knn_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False)),
    ("knn", KNeighborsClassifier(n_neighbors=11, weights="distance", p=2, n_jobs=1)),
])
knn_pipe.fit(X_train, y_train)
evaluate_and_print("KNN baseline (k=11, distance, p=2)", y_test, knn_pipe.predict(X_test))

# Small GridSearchCV on full train (3-fold x 4 candidates)
param_grid = {"knn__n_neighbors":[5,11], "knn__weights":["uniform","distance"], "knn__p":[2]}
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)
grid = GridSearchCV(estimator=knn_pipe, param_grid=param_grid, scoring="f1_macro",
                    cv=cv, n_jobs=1, pre_dispatch=1, verbose=2, refit=True)
grid.fit(X_train, y_train)
print("\nBest KNN params:", grid.best_params_)
evaluate_and_print("KNN (GridSearch best - small/full)", y_test, grid.predict(X_test))

# Logistic Regression (raise max_iter to avoid ConvergenceWarning)
lin_pipe = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler(with_mean=False)),
    ("lr", LogisticRegression(max_iter=1000, solver="lbfgs", random_state=RANDOM_STATE)),
])
lin_pipe.fit(X_train, y_train)
evaluate_and_print("Logistic Regression", y_test, lin_pipe.predict(X_test))
```
